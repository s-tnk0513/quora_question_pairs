{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===Get Data====\n",
    "\n",
    "#Get train data\n",
    "df_train = pd.read_csv('../data/given_data/train.csv')\n",
    "\n",
    "#Get test data\n",
    "df_test= pd.read_csv(\"../data/given_data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===Processing===\n",
    "\n",
    "#Create stopword List\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===word_match_share===\n",
    "\n",
    "#Calculate Jaccard　coefficient between two questions with word count.\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply df_train data to word_match_function\n",
    "word_match_train = df_train.apply(word_match_share, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===tfidf_word_match_share====\n",
    "\n",
    "#Create word list\n",
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "\n",
    "#Calculate weight\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "eps = 5000 \n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Jaccard　coefficient between two questions with tfidf word count.\n",
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanakasho/.pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "#Apply df_train data to word_match_function\n",
    "tfidf_word_match_train = df_train.apply(tfidf_word_match_share, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===word_count_distance===\n",
    "\n",
    "#Count word_count_distance\n",
    "def word_count_distance(row):\n",
    "    count_word = len(str(row[\"question1\"]).lower().split()) - len(str(row[\"question2\"]).lower().split())\n",
    "    return np.linalg.norm(count_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply df_train data to word_match function\n",
    "word_count_distance_train = df_train.apply(word_count_distance, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize train dataframe\n",
    "x_train = pd.DataFrame()\n",
    "y_train = pd.DataFrame()\n",
    "\n",
    "#Add the column of  predictor valiable(explained valiable)\n",
    "x_train[\"word_match\"] = word_match_train\n",
    "x_train[\"tfidf_word_match\"] = tfidf_word_match_train\n",
    "x_train[\"word_count_distance\"] = word_count_distance_train\n",
    "\n",
    "#Add the column of objective valiable\n",
    "y_train[\"is_duplicate\"] = df_train.is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Save x_train\n",
    "x_train.to_csv(\"../data/train_data/x_train.csv\",index = False)\n",
    "\n",
    "#Save y_train\n",
    "y_train.to_csv(\"../data/train_data/y_train.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load x_train and y_train(from the second time)\n",
    "x_train = pd.read_csv(\"../data/train_data/x_train.csv\")\n",
    "\n",
    "y_train = pd.read_csv(\"../data/train_data/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanakasho/.pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/tanakasho/.pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "#===test data====\n",
    "\n",
    "#Apply df_test data to word_match_share function\n",
    "word_match_test= df_test.apply(word_match_share, axis=1, raw=True)\n",
    "\n",
    "#Apply df_test data to tfidf_word_match_share function\n",
    "tfidf_word_match_test = df_test.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "\n",
    "#Apply df_test data to word_count_distance function\n",
    "word_count_distance_test= df_test.apply(word_count_distance, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize test dataframe\n",
    "x_test = pd.DataFrame()\n",
    "y_test = pd.DataFrame()\n",
    "\n",
    "#Add the column of  predictor valiable(explained valiable)\n",
    "x_test[\"word_match\"] = word_match_test\n",
    "x_test[\"tfidf_word_match\"] = tfidf_word_match_test\n",
    "x_test[\"word_count_distance\"] = word_count_distance_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save x_test\n",
    "x_test.to_csv(\"../data/test_data/x_test.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load x_test(from the second time)\n",
    "x_test = pd.read_csv(\"../data/test_data/x_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19124366100096607\n"
     ]
    }
   ],
   "source": [
    "y_train = df_train['is_duplicate'].values\n",
    "pos_train = x_train[y_train == 1]\n",
    "neg_train = x_train[y_train == 0]\n",
    "\n",
    "# Now we oversample the negative class\n",
    "# There is likely a much more elegant way to do this...\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "x_train = pd.concat([pos_train, neg_train])\n",
    "y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we split some of the data off for validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanakasho/.pyenv/versions/3.6.2/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 0.639237\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 0.600759\n",
      "[3]\tvalid_0's multi_logloss: 0.568751\n",
      "[4]\tvalid_0's multi_logloss: 0.54181\n",
      "[5]\tvalid_0's multi_logloss: 0.518944\n",
      "[6]\tvalid_0's multi_logloss: 0.499353\n",
      "[7]\tvalid_0's multi_logloss: 0.482507\n",
      "[8]\tvalid_0's multi_logloss: 0.467958\n",
      "[9]\tvalid_0's multi_logloss: 0.455318\n",
      "[10]\tvalid_0's multi_logloss: 0.444285\n",
      "[11]\tvalid_0's multi_logloss: 0.434646\n",
      "[12]\tvalid_0's multi_logloss: 0.426172\n",
      "[13]\tvalid_0's multi_logloss: 0.41875\n",
      "[14]\tvalid_0's multi_logloss: 0.412223\n",
      "[15]\tvalid_0's multi_logloss: 0.406433\n",
      "[16]\tvalid_0's multi_logloss: 0.401353\n",
      "[17]\tvalid_0's multi_logloss: 0.396861\n",
      "[18]\tvalid_0's multi_logloss: 0.392865\n",
      "[19]\tvalid_0's multi_logloss: 0.389295\n",
      "[20]\tvalid_0's multi_logloss: 0.386168\n",
      "[21]\tvalid_0's multi_logloss: 0.383343\n",
      "[22]\tvalid_0's multi_logloss: 0.38082\n",
      "[23]\tvalid_0's multi_logloss: 0.378579\n",
      "[24]\tvalid_0's multi_logloss: 0.37658\n",
      "[25]\tvalid_0's multi_logloss: 0.37479\n",
      "[26]\tvalid_0's multi_logloss: 0.373212\n",
      "[27]\tvalid_0's multi_logloss: 0.371804\n",
      "[28]\tvalid_0's multi_logloss: 0.370523\n",
      "[29]\tvalid_0's multi_logloss: 0.369403\n",
      "[30]\tvalid_0's multi_logloss: 0.368386\n",
      "[31]\tvalid_0's multi_logloss: 0.367499\n",
      "[32]\tvalid_0's multi_logloss: 0.366695\n",
      "[33]\tvalid_0's multi_logloss: 0.36598\n",
      "[34]\tvalid_0's multi_logloss: 0.365321\n",
      "[35]\tvalid_0's multi_logloss: 0.364747\n",
      "[36]\tvalid_0's multi_logloss: 0.364217\n",
      "[37]\tvalid_0's multi_logloss: 0.363751\n",
      "[38]\tvalid_0's multi_logloss: 0.363334\n",
      "[39]\tvalid_0's multi_logloss: 0.362981\n",
      "[40]\tvalid_0's multi_logloss: 0.362613\n",
      "[41]\tvalid_0's multi_logloss: 0.362319\n",
      "[42]\tvalid_0's multi_logloss: 0.362025\n",
      "[43]\tvalid_0's multi_logloss: 0.361784\n",
      "[44]\tvalid_0's multi_logloss: 0.361552\n",
      "[45]\tvalid_0's multi_logloss: 0.36136\n",
      "[46]\tvalid_0's multi_logloss: 0.361165\n",
      "[47]\tvalid_0's multi_logloss: 0.361009\n",
      "[48]\tvalid_0's multi_logloss: 0.360865\n",
      "[49]\tvalid_0's multi_logloss: 0.360741\n",
      "[50]\tvalid_0's multi_logloss: 0.360601\n",
      "[51]\tvalid_0's multi_logloss: 0.360474\n",
      "[52]\tvalid_0's multi_logloss: 0.360377\n",
      "[53]\tvalid_0's multi_logloss: 0.360264\n",
      "[54]\tvalid_0's multi_logloss: 0.360183\n",
      "[55]\tvalid_0's multi_logloss: 0.36012\n",
      "[56]\tvalid_0's multi_logloss: 0.360008\n",
      "[57]\tvalid_0's multi_logloss: 0.359921\n",
      "[58]\tvalid_0's multi_logloss: 0.359858\n",
      "[59]\tvalid_0's multi_logloss: 0.359789\n",
      "[60]\tvalid_0's multi_logloss: 0.359746\n",
      "[61]\tvalid_0's multi_logloss: 0.359702\n",
      "[62]\tvalid_0's multi_logloss: 0.359625\n",
      "[63]\tvalid_0's multi_logloss: 0.359568\n",
      "[64]\tvalid_0's multi_logloss: 0.359512\n",
      "[65]\tvalid_0's multi_logloss: 0.359483\n",
      "[66]\tvalid_0's multi_logloss: 0.359503\n",
      "[67]\tvalid_0's multi_logloss: 0.359452\n",
      "[68]\tvalid_0's multi_logloss: 0.359416\n",
      "[69]\tvalid_0's multi_logloss: 0.359342\n",
      "[70]\tvalid_0's multi_logloss: 0.359555\n",
      "[71]\tvalid_0's multi_logloss: 0.359329\n",
      "[72]\tvalid_0's multi_logloss: 0.359313\n",
      "[73]\tvalid_0's multi_logloss: 0.359299\n",
      "[74]\tvalid_0's multi_logloss: 0.359272\n",
      "[75]\tvalid_0's multi_logloss: 0.359221\n",
      "[76]\tvalid_0's multi_logloss: 0.359207\n",
      "[77]\tvalid_0's multi_logloss: 0.359243\n",
      "[78]\tvalid_0's multi_logloss: 0.359256\n",
      "[79]\tvalid_0's multi_logloss: 0.359225\n",
      "[80]\tvalid_0's multi_logloss: 0.359197\n",
      "[81]\tvalid_0's multi_logloss: 0.359197\n",
      "[82]\tvalid_0's multi_logloss: 0.359271\n",
      "[83]\tvalid_0's multi_logloss: 0.359242\n",
      "[84]\tvalid_0's multi_logloss: 0.35962\n",
      "[85]\tvalid_0's multi_logloss: 0.359174\n",
      "[86]\tvalid_0's multi_logloss: 0.360583\n",
      "[87]\tvalid_0's multi_logloss: 0.360284\n",
      "[88]\tvalid_0's multi_logloss: 0.359621\n",
      "[89]\tvalid_0's multi_logloss: 0.360011\n",
      "[90]\tvalid_0's multi_logloss: 0.359151\n",
      "[91]\tvalid_0's multi_logloss: 0.359153\n",
      "[92]\tvalid_0's multi_logloss: 0.360003\n",
      "[93]\tvalid_0's multi_logloss: 0.35911\n",
      "[94]\tvalid_0's multi_logloss: 0.360287\n",
      "[95]\tvalid_0's multi_logloss: 0.359086\n",
      "[96]\tvalid_0's multi_logloss: 0.359081\n",
      "[97]\tvalid_0's multi_logloss: 0.359082\n",
      "[98]\tvalid_0's multi_logloss: 0.360265\n",
      "[99]\tvalid_0's multi_logloss: 0.360397\n",
      "[100]\tvalid_0's multi_logloss: 0.35906\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.35906\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'multiclass',\n",
    "        'metric': {'multi_logloss'},\n",
    "        'num_class': 3,\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 23,\n",
    "        'min_data_in_leaf': 1,\n",
    "        'num_iteration': 100,\n",
    "        'verbose': 0\n",
    "}\n",
    "\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "            lgb_train,\n",
    "            num_boost_round=50,\n",
    "            valid_sets=lgb_eval,\n",
    "            early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict y_test with x_test\n",
    "y_test = gbm.predict(x_test, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize submission DataFrame\n",
    "submission=pd.DataFrame()\n",
    "\n",
    "#Add the column of test_id\n",
    "submission['test_id']=df_test['test_id']\n",
    "\n",
    "#Add the column of 0-class probablity\n",
    "submission['is_duplicate']=y_test[:,1]\n",
    "\n",
    "#Save submission\n",
    "submission.to_csv(\"../submission/submission_DL.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
